# =========================
# üìä Pareto Comunidad ‚Äì MSP (1 archivo, cat√°logo embebido y optimizado)
# =========================
# Flujo:
# 1) Subir Plantilla de Comunidad (hoja 'matriz').
# 2) Generar 'Copilado Comunidad' (Descriptor, Frecuencia) con cat√°logo embebido.
# 3) Generar 'Pareto Comunidad' (Categor√≠a, Descriptor, Frecuencia, Porcentaje, % Acumulado, Acumulado, 80/20).
# 4) Ver gr√°fico (barras + l√≠nea) con corte al 80% y descargar Excel con todo y gr√°fico.
#
# Rendimiento:
# - UI pinta de inmediato.
# - Preview limitada (no cuelga el frontend).
# - Matching por encabezado r√°pido; escaneo profundo de texto opcional (toggle).
# - Escaneo profundo vectorizado con regex y por bloques.
# - Gr√°fico limitado a TOP_N_GRAFICO (la descarga incluye TODOS).
#
# Cat√°logo embebido:
# - Basado en descriptores t√≠picos. Puedes ampliarlo pegando l√≠neas CSV (NOMBRE CORTO,CATEGOR√çA,DESCRIPTOR).

import re
import unicodedata
from io import BytesIO
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

st.set_page_config(page_title="Pareto Comunidad ‚Äì MSP", layout="wide", initial_sidebar_state="collapsed")

TOP_N_GRAFICO = 40  # limitar gr√°fico (la descarga incluye todos)

# -------------------------
# Cat√°logo embebido (ampl√≠a si ocupas)
# -------------------------
CATALOGO_BASE = [
    # (NOMBRE CORTO, CATEGOR√çA, DESCRIPTOR)
    ("HURTO", "DELITOS CONTRA LA PROPIEDAD", "HURTO"),
    ("ROBO", "DELITOS CONTRA LA PROPIEDAD", "ROBO"),
    ("DA√ëOS A LA PROPIEDAD", "DELITOS CONTRA LA PROPIEDAD", "DA√ëOS A LA PROPIEDAD"),
    ("ASALTO", "DELITOS CONTRA LA PROPIEDAD", "ASALTO"),
    ("TENTATIVA DE ROBO", "DELITOS CONTRA LA PROPIEDAD", "TENTATIVA DE ROBO"),

    ("VENTA DE DROGAS", "DROGAS", "VENTA DE DROGAS"),
    ("TRAFICO DE DROGAS", "DROGAS", "TR√ÅFICO DE DROGAS"),
    ("MICROTRAFICO", "DROGAS", "MICROTR√ÅFICO"),
    ("CONSUMO DE DROGAS", "DROGAS", "CONSUMO DE DROGAS"),
    ("BUNKER", "DROGAS", "B√öNKER"),
    ("PUNTO DE VENTA", "DROGAS", "PUNTO DE VENTA"),
    ("CONSUMO DE ALCOHOL", "ALCOHOL", "CONSUMO DE ALCOHOL EN V√çA P√öBLICA"),
    ("LICORES", "ALCOHOL", "CONSUMO DE ALCOHOL EN V√çA P√öBLICA"),

    ("HOMICIDIOS", "DELITOS CONTRA LA VIDA", "HOMICIDIOS"),
    ("HERIDOS", "DELITOS CONTRA LA VIDA", "HERIDOS"),
    ("TENTATIVA DE HOMICIDIO", "DELITOS CONTRA LA VIDA", "TENTATIVA DE HOMICIDIO"),

    ("VIOLENCIA DOMESTICA", "VIOLENCIA", "VIOLENCIA DOM√âSTICA"),
    ("VIOLENCIA INTRAFAMILIAR", "VIOLENCIA", "VIOLENCIA DOM√âSTICA"),
    ("AGRESION", "VIOLENCIA", "AGRESI√ìN"),
    ("ABUSO SEXUAL", "VIOLENCIA", "ABUSO SEXUAL"),
    ("VIOLACION", "VIOLENCIA", "VIOLACI√ìN"),
    ("ACOSO SEXUAL CALLEJERO", "RIESGO SOCIAL", "ACOSO SEXUAL CALLEJERO"),
    ("ACOSO ESCOLAR", "RIESGO SOCIAL", "ACOSO ESCOLAR (BULLYING)"),
    ("ACTOS OBSCENOS", "RIESGO SOCIAL", "ACTOS OBSCENOS EN VIA PUBLICA"),

    ("PANDILLAS", "ORDEN P√öBLICO", "PANDILLAS"),
    ("VAGANCIA", "ORDEN P√öBLICO", "VAGANCIA"),
    ("INDIGENCIA", "ORDEN P√öBLICO", "INDIGENCIA"),
    ("RUIDO", "ORDEN P√öBLICO", "CONTAMINACI√ìN SONORA"),
    ("CARRERAS ILEGALES", "ORDEN P√öBLICO", "CARRERAS ILEGALES"),
    ("ARMAS BLANCAS", "ORDEN P√öBLICO", "PORTACI√ìN DE ARMA BLANCA"),
]

# -------------------------
# Utils de normalizaci√≥n
# -------------------------
def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def norm(s: Optional[str]) -> str:
    if s is None:
        return ""
    if not isinstance(s, str):
        s = str(s)
    s = strip_accents(s).strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def split_tokens(txt: str) -> List[str]:
    if not isinstance(txt, str):
        return []
    parts = re.split(r"[;,/|]+", txt)
    return [p.strip() for p in parts if p and p.strip()]

def preview_df(df: pd.DataFrame, n: int = 200) -> pd.DataFrame:
    return df.head(n).copy()

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()
    df2.columns = [
        re.sub(r"\s+", " ", strip_accents(str(c)).strip().lower())
        for c in df2.columns
    ]
    return df2

# -------------------------
# UI
# -------------------------
st.title("Pareto Comunidad (MSP) ‚Äì Cat√°logo embebido")

with st.expander("Instrucciones", expanded=True):
    st.markdown(f"""
1) **Sube la Plantilla de Comunidad** (XLSX, hoja `matriz`).  
2) (Opcional) **Ampli√° el cat√°logo** pegando filas en el panel de abajo (`NOMBRE CORTO,CATEGOR√çA,DESCRIPTOR`).  
3) Gener√° **Copilado** ‚Üí **Pareto** ‚Üí **Descarg√° Excel** con gr√°fico.  

> El gr√°fico muestra **top {TOP_N_GRAFICO}** por rendimiento; la descarga incluye **todos**.
""")

plantilla_file = st.file_uploader("üìÑ Plantilla de Comunidad (XLSX) ‚Äì hoja 'matriz'", type=["xlsx"], key="plantilla")
st.divider()

with st.expander("‚ûï Ampliar/actualizar cat√°logo (pegar l√≠neas CSV)", expanded=False):
    st.markdown("Formato por l√≠nea: `NOMBRE CORTO,CATEGOR√çA,DESCRIPTOR` (sin encabezados).")
    pasted = st.text_area("Pega aqu√≠ (opcional):", height=150, placeholder="Ejemplo:\nHURTO,DELITOS CONTRA LA PROPIEDAD,HURTO\nVENTA DE DROGAS,DROGAS,VENTA DE DROGAS")
    def parse_pasted(text: str) -> List[tuple]:
        rows = []
        for line in text.splitlines():
            line = line.strip()
            if not line:
                continue
            parts = [p.strip() for p in line.split(",")]
            if len(parts) < 3:
                continue
            nc, cat, desc = parts[0], parts[1], ",".join(parts[2:]).strip()
            rows.append((nc, cat, desc))
        return rows
    pasted_rows = parse_pasted(pasted) if pasted else []

# -------------------------
# Lectura de la plantilla
# -------------------------
@st.cache_data(show_spinner=False)
def read_plantilla_matriz(file) -> pd.DataFrame:
    # Leer completo; preview se limita en UI
    return pd.read_excel(file, sheet_name="matriz", engine="openpyxl")

# -------------------------
# Construcci√≥n del cat√°logo activo
# -------------------------
def build_catalogo_df() -> pd.DataFrame:
    base = pd.DataFrame(CATALOGO_BASE, columns=["NOMBRE CORTO", "CATEGOR√çA", "DESCRIPTOR"])
    if pasted_rows:
        extra = pd.DataFrame(pasted_rows, columns=["NOMBRE CORTO", "CATEGOR√çA", "DESCRIPTOR"])
        cat_df = pd.concat([base, extra], ignore_index=True)
        cat_df = cat_df.dropna(subset=["DESCRIPTOR"])
        # deduplicar por DESCRIPTOR, preferimos lo √∫ltimo pegado
        cat_df = cat_df.drop_duplicates(subset=["DESCRIPTOR"], keep="last")
        return cat_df
    return base

def build_keyword_maps(desc_df: pd.DataFrame) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, str]]:
    by_desc_norm: Dict[str, str] = {}
    by_nc_norm: Dict[str, str] = {}
    cat_by_desc: Dict[str, str] = {}
    for _, r in desc_df.iterrows():
        desc = str(r["DESCRIPTOR"]).strip()
        cat = str(r["CATEGOR√çA"]).strip() if "CATEGOR√çA" in r and not pd.isna(r["CATEGOR√çA"]) else ""
        nc  = str(r["NOMBRE CORTO"]).strip() if "NOMBRE CORTO" in desc_df.columns and not pd.isna(r["NOMBRE CORTO"]) else ""
        if desc:
            by_desc_norm[norm(desc)] = desc
            cat_by_desc[desc] = cat
        if nc:
            by_nc_norm[norm(nc)] = desc
    return by_desc_norm, by_nc_norm, cat_by_desc

# -------------------------
# Extracci√≥n de descriptores (r√°pido + profundo opcional)
# -------------------------
def detect_descriptors_from_dataframe(df_raw: pd.DataFrame,
                                      by_desc_norm: Dict[str, str],
                                      by_nc_norm: Dict[str, str],
                                      deep_scan: bool = False) -> List[str]:
    df = normalize_columns(df_raw)

    keys_desc = list(by_desc_norm.keys())
    keys_nc   = list(by_nc_norm.keys())

    def header_to_descriptor(ncol: str) -> Optional[str]:
        for k in keys_desc:
            if k and (k == ncol or k in ncol or ncol in k):
                return by_desc_norm[k]
        for k in keys_nc:
            if k and (k == ncol or k in ncol or ncol in k):
                return by_nc_norm[k]
        return None

    col_map: Dict[str, str] = {}
    for col in df.columns:
        d = header_to_descriptor(col)
        if d:
            col_map[col] = d

    def is_marked_series(s: pd.Series) -> pd.Series:
        s2 = s.copy()
        numeric_mask = pd.to_numeric(s2, errors="coerce").fillna(0) != 0
        txt = s2.astype(str).str.strip().str.lower().apply(strip_accents)
        text_mask = ~txt.isin(["", "no", "0", "nan", "none", "false"])
        return numeric_mask | text_mask

    hits: List[str] = []

    # Conteo r√°pido por columnas mapeadas
    for col, desc in col_map.items():
        try:
            mask = is_marked_series(df[col])
            count = int(mask.sum())
            if count > 0:
                hits.extend([desc] * count)
        except Exception:
            continue

    if not deep_scan:
        return hits

    # Escaneo profundo en texto
    all_keys = list(set(keys_desc + keys_nc))
    all_keys.sort(key=len, reverse=True)
    pattern = "|".join([re.escape(k) for k in all_keys if k])
    if not pattern:
        return hits
    regex = re.compile(pattern)

    text_cols = []
    for col in df.columns:
        col_series = df[col]
        if col_series.dtype == object:
            sample = col_series.head(200).astype(str)
            if (sample.str.strip() != "").mean() > 0.05:
                text_cols.append(col)

    def norm_text_cell(x: str) -> str:
        x = strip_accents(x)
        x = re.sub(r"\s+", " ", x.strip().lower())
        return x

    block_size = 5000
    for col in text_cols:
        col_norm = df[col].astype(str).apply(norm_text_cell)
        for i in range(0, len(col_norm), block_size):
            part = col_norm.iloc[i:i+block_size]
            matched_idx = part[part.str.contains(regex, na=False)].index
            if len(matched_idx) == 0:
                continue
            for ridx in matched_idx:
                txt = part.loc[ridx]
                m = regex.search(txt)
                if not m:
                    continue
                key = m.group(0)
                desc = by_desc_norm.get(key) or by_nc_norm.get(key)
                if desc:
                    hits.append(desc)

    return hits

# -------------------------
# Agregaciones y Pareto
# -------------------------
def make_copilado(hits: List[str]) -> pd.DataFrame:
    if not hits:
        return pd.DataFrame({"Descriptor": [], "Frecuencia": []})
    s = pd.Series(hits, name="Descriptor")
    df = s.value_counts(dropna=False).rename_axis("Descriptor").reset_index(name="Frecuencia")
    df = df.sort_values(["Frecuencia", "Descriptor"], ascending=[False, True], ignore_index=True)
    return df

def make_pareto(copilado_df: pd.DataFrame, cat_by_desc: Dict[str, str]) -> pd.DataFrame:
    if copilado_df.empty:
        return pd.DataFrame(columns=[
            "Categor√≠a", "Descriptor", "Frecuencia", "Porcentaje", "% Acumulado", "Acumulado", "80/20"
        ])
    df = copilado_df.copy()
    df["Categor√≠a"] = df["Descriptor"].map(cat_by_desc).fillna("")
    total = df["Frecuencia"].sum()
    df["Porcentaje"] = (df["Frecuencia"] / total) * 100.0
    df = df.sort_values(["Frecuencia", "Descriptor"], ascending=[False, True], ignore_index=True)
    df["% Acumulado"] = df["Porcentaje"].cumsum()
    df["Acumulado"] = df["Frecuencia"].cumsum()
    df["80/20"] = np.where(df["% Acumulado"] <= 80.0, "‚â§80%", ">80%")
    return df[["Categor√≠a", "Descriptor", "Frecuencia", "Porcentaje", "% Acumulado", "Acumulado", "80/20"]]

def plot_pareto(df_pareto: pd.DataFrame):
    if df_pareto.empty:
        return go.Figure()
    x = df_pareto["Descriptor"].astype(str).tolist()
    y = df_pareto["Frecuencia"].tolist()
    cum = df_pareto["% Acumulado"].tolist()
    fig = go.Figure()
    fig.add_bar(x=x, y=y, name="Frecuencia", yaxis="y1")
    fig.add_trace(go.Scatter(x=x, y=cum, name="% Acumulado", yaxis="y2", mode="lines+markers"))
    fig.add_trace(go.Scatter(x=x, y=[80.0]*len(x), name="Corte 80%", yaxis="y2", mode="lines", line=dict(dash="dash")))
    fig.update_layout(
        title="Pareto Comunidad",
        xaxis_title="Descriptor",
        yaxis=dict(title="Frecuencia"),
        yaxis2=dict(title="% Acumulado", overlaying="y", side="right", range=[0, 100]),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=20, r=20, t=50, b=80),
        hovermode="x unified"
    )
    return fig

# -------------------------
# Exportar a Excel con gr√°fico
# -------------------------
def build_excel_bytes(copilado_df: pd.DataFrame, pareto_df: pd.DataFrame) -> bytes:
    from pandas import ExcelWriter
    import xlsxwriter  # noqa: F401  # usado por engine
    output = BytesIO()
    with ExcelWriter(output, engine="xlsxwriter") as writer:
        copilado_df.to_excel(writer, index=False, sheet_name="Copilado Comunidad")
        pareto_df.to_excel(writer, index=False, sheet_name="Pareto Comunidad")

        wb  = writer.book
        wsP = writer.sheets["Pareto Comunidad"]
        n = len(pareto_df)
        if n >= 1:
            # (A) Categor√≠a, (B) Descriptor, (C) Frecuencia, (D) Porcentaje, (E) % Acumulado, (F) Acumulado, (G) 80/20
            chart = wb.add_chart({'type': 'column'})
            chart.add_series({
                'name':       'Frecuencia',
                'categories': ['Pareto Comunidad', 1, 1, n, 1],  # B
                'values':     ['Pareto Comunidad', 1, 2, n, 2],  # C
            })
            line_chart = wb.add_chart({'type': 'line'})
            line_chart.add_series({
                'name':       '% Acumulado',
                'categories': ['Pareto Comunidad', 1, 1, n, 1],  # B
                'values':     ['Pareto Comunidad', 1, 4, n, 4],  # E
                'y2_axis':    True,
                'marker':     {'type': 'automatic'}
            })
            chart.combine(line_chart)
            chart.set_title({'name': 'Pareto Comunidad'})
            chart.set_x_axis({'name': 'Descriptor'})
            chart.set_y_axis({'name': 'Frecuencia'})
            chart.set_y2_axis({'name': '% Acumulado', 'major_gridlines': {'visible': False}, 'min': 0, 'max': 100})
            wsP.insert_chart(1, 9, chart, {'x_scale': 1.3, 'y_scale': 1.3})
    return output.getvalue()

# -------------------------
# MAIN
# -------------------------
if plantilla_file:
    with st.spinner("Leyendo archivo‚Ä¶"):
        try:
            df_matriz = read_plantilla_matriz(plantilla_file)
        except Exception as e:
            st.error(f"Error al leer la Plantilla: {e}")
            st.stop()

    st.subheader("1) Previsualizaci√≥n")
    st.caption("Primeras filas de la hoja `matriz` (preview limitada)")
    st.dataframe(preview_df(df_matriz), use_container_width=True)

    cat_df = build_catalogo_df()
    st.caption("Cat√°logo activo (primeras filas)")
    st.dataframe(cat_df.head(20), use_container_width=True)

    by_desc_norm, by_nc_norm, cat_by_desc = build_keyword_maps(cat_df)

    st.subheader("2) Generar 'Copilado Comunidad'")
    deep_scan = st.toggle(
        "Activar escaneo profundo de texto (m√°s lento)",
        value=False,
        help="Si est√° apagado, solo se detecta por encabezados y celdas marcadas. Enci√©ndelo si necesitas raspar menciones en texto libre."
    )
    with st.spinner("Procesando‚Ä¶"):
        hits = detect_descriptors_from_dataframe(df_matriz, by_desc_norm, by_nc_norm, deep_scan=deep_scan)
        copilado_df = make_copilado(hits)

    if copilado_df.empty:
        st.warning("No se detectaron descriptores con el cat√°logo actual. Puedes ampliarlo pegando filas en el expander superior.")
        st.stop()

    st.success("Copilado generado.")
    st.dataframe(copilado_df, use_container_width=True)

    st.subheader("3) Generar 'Pareto Comunidad'")
    pareto_df = make_pareto(copilado_df, cat_by_desc)
    st.dataframe(pareto_df, use_container_width=True)

    st.subheader("4) Gr√°fico Pareto (con corte al 80%)")
    pareto_df_plot = pareto_df.head(TOP_N_GRAFICO).copy()
    fig = plot_pareto(pareto_df_plot)
    st.plotly_chart(fig, use_container_width=True)
    st.caption(f"Mostrando top {len(pareto_df_plot)} por frecuencia. La descarga incluye **todos** los descriptores.")

    st.subheader("5) Descarga")
    xls_bytes = build_excel_bytes(copilado_df, pareto_df)
    st.download_button(
        label="‚¨áÔ∏è Descargar Excel (Copilado + Pareto + Gr√°fico)",
        data=xls_bytes,
        file_name="Pareto_Comunidad.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
else:
    st.info("Sube la **Plantilla de Comunidad** (XLSX, hoja `matriz`) para comenzar. Puedes ampliar el cat√°logo en el expander si lo necesitas.")

